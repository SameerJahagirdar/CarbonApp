{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SameerJahagirdar/CarbonApp/blob/master/Carbon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!pip install tesseract"
      ],
      "metadata": {
        "id": "pzYPSn2oyrP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "6d8999e4-22a7-41b8-8aed-7c53c51629a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting Pillow>=8.0.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from pytesseract) (23.0)\n",
            "Installing collected packages: Pillow, pytesseract\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed Pillow-9.4.0 pytesseract-0.3.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tesseract\n",
            "  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tesseract\n",
            "  Building wheel for tesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562571 sha256=023f89358648683ba64e00cd72aa5ddc54179a67b598c306a3b69494f20c85c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/a1/69/fabe07004553a36d818e4657fed410daf525fe1ae161f469d3\n",
            "Successfully built tesseract\n",
            "Installing collected packages: tesseract\n",
            "Successfully installed tesseract-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tesseract-ocr\n"
      ],
      "metadata": {
        "id": "nSqI3SFt5jzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a3f68e-2ad5-4bf8-caa6-392a5e1a3b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 4,850 kB of archives.\n",
            "After this operation, 16.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1 [1,598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr amd64 4.1.1-2build2 [262 kB]\n",
            "Fetched 4,850 kB in 1s (4,851 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2build2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2build2) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2build2) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract==0.3.9"
      ],
      "metadata": {
        "id": "SP6NacM96Ite",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0ee114-b3f0-4232-e55d-a49382968d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytesseract==0.3.9\n",
            "  Downloading pytesseract-0.3.9-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from pytesseract==0.3.9) (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from pytesseract==0.3.9) (23.0)\n",
            "Installing collected packages: pytesseract\n",
            "  Attempting uninstall: pytesseract\n",
            "    Found existing installation: pytesseract 0.3.10\n",
            "    Uninstalling pytesseract-0.3.10:\n",
            "      Successfully uninstalled pytesseract-0.3.10\n",
            "Successfully installed pytesseract-0.3.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwxwZTL5xjCI"
      },
      "outputs": [],
      "source": [
        "# import pytesseract\n",
        "# import cv2\n",
        "\n",
        "# # Load the image\n",
        "# img = cv2.imread('meter2.jpg')\n",
        "\n",
        "# # Convert the image to grayscale\n",
        "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# # Apply thresholding to convert the image to binary\n",
        "# _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "\n",
        "# # Apply some image processing to improve character recognition\n",
        "# kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
        "# opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "# closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "\n",
        "# # Extract the meter reading using Tesseract OCR\n",
        "# meter_reading = pytesseract.image_to_string(closing, config='--psm 6')\n",
        "# print(\"Meter reading:\", meter_reading)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "# Load the image\n",
        "img = Image.open('meter.jpg')\n",
        "\n",
        "# Preprocess the image (optional)\n",
        "# This could include resizing, cropping, enhancing, or other techniques to improve OCR accuracy\n",
        "img = img.resize((800, 600))\n",
        "# img1=ImageChops.invert(img)\n",
        "# Extract text from the image using pytesseract\n",
        "data = pytesseract.image_to_string(img)\n",
        "\n",
        "# Parse the data\n",
        "# Depending on the format of the text, you may need to split it into individual data fields\n",
        "data_fields = data.split('\\n')\n",
        "print(data_fields)\n",
        "# Store the data\n",
        "with open('gas_readings.csv', 'a') as f:\n",
        "    f.write(','.join(data_fields) + '\\n')\n"
      ],
      "metadata": {
        "id": "o0GLalrEyp4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-vision"
      ],
      "metadata": {
        "id": "NIAHJNK3rAIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# import os\n",
        "\n",
        "# # Imports the Google Cloud client library\n",
        "# from google.cloud import vision\n",
        "\n",
        "# # Instantiates a client\n",
        "# client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# # The name of the image file to annotate\n",
        "# file_name = os.path.abspath('/content/meterw.jpg')\n",
        "\n",
        "# # Loads the image into memory\n",
        "# with io.open(file_name, 'rb') as image_file:\n",
        "#     content = image_file.read()\n",
        "\n",
        "# image = vision.Image(content=content)\n",
        "\n",
        "# # Performs label detection on the image file\n",
        "# response = client.label_detection(image=image)\n",
        "# labels = response.label_annotations\n",
        "\n",
        "# print('Labels:')\n",
        "# for label in labels:\n",
        "#     print(label.description)"
      ],
      "metadata": {
        "id": "P9Zr2V0zrJl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# import cv2\n",
        "# import imutils\n",
        "# import json\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import requests\n",
        "# import time\n",
        "# from base64 import b64encode\n",
        "# from IPython.display import Image\n",
        "# from pylab import rcParams\n",
        "     \n",
        "\n",
        "# rcParams['figure.figsize'] = 10, 20\n",
        "     \n",
        "\n",
        "# def makeImageData(imgpath):\n",
        "#     img_req = None\n",
        "#     with open(imgpath, 'rb') as f:\n",
        "#         ctxt = b64encode(f.read()).decode()\n",
        "#         img_req = {\n",
        "#             'image': {\n",
        "#                 'content': ctxt\n",
        "#             },\n",
        "#             'features': [{\n",
        "#                 'type': 'DOCUMENT_TEXT_DETECTION',\n",
        "#                 'maxResults': 1\n",
        "#             }]\n",
        "#         }\n",
        "#     return json.dumps({\"requests\": img_req}).encode()\n",
        "     \n",
        "\n",
        "# def requestOCR(url, api_key, imgpath):\n",
        "#   imgdata = makeImageData(imgpath)\n",
        "#   response = requests.post(ENDPOINT_URL, \n",
        "#                            data = imgdata, \n",
        "#                            params = {'key': api_key}, \n",
        "#                            headers = {'Content-Type': 'application/json'})\n",
        "#   return response\n",
        "     \n",
        "\n",
        "# with open('vision_api.json') as f:\n",
        "#     data = json.load(f)\n",
        "     \n",
        "\n",
        "# ENDPOINT_URL = 'https://vision.googleapis.com/v1/images:annotate'\n",
        "# api_key = data[\"api_key\"]\n",
        "# img_loc = \"Image.jpg\"\n",
        "     \n",
        "\n",
        "# Image(img_loc)\n",
        "     \n",
        "\n",
        "# result = requestOCR(ENDPOINT_URL, api_key, img_loc)\n",
        "     \n",
        "\n",
        "# if result.status_code != 200 or result.json().get('error'):\n",
        "#     print (\"Error\")\n",
        "# else:\n",
        "#     result = result.json()['responses'][0]['textAnnotations']\n",
        "     \n",
        "\n",
        "# result\n",
        "     \n",
        "\n",
        "# for index in range(len(result)):\n",
        "#   print(result[index][\"description\"])\n",
        "     \n",
        "\n",
        "# def gen_cord(result):\n",
        "#   cord_df = pd.DataFrame(result['boundingPoly']['vertices'])\n",
        "#   x_min, y_min = np.min(cord_df[\"x\"]), np.min(cord_df[\"y\"])\n",
        "#   x_max, y_max = np.max(cord_df[\"x\"]), np.max(cord_df[\"y\"])\n",
        "#   return result[\"description\"], x_max, x_min, y_max, y_min\n",
        "     \n",
        "\n",
        "# text, x_max, x_min, y_max, y_min = gen_cord(result[-1])\n",
        "# image = cv2.imread(img_loc)\n",
        "# cv2.rectangle(image,(x_min,y_min),(x_max,y_max),(0,255, 0),2)\n",
        "# plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "# print (\"Text Detected = {}\".format(text))\n",
        "     \n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "dTYhz0NQ_iNq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Set the API key and endpoint URL\n",
        "subscription_key = '3095b4d700184ffb81f5f991c9ee51ab'\n",
        "endpoint = 'https://siddhikottawar232.cognitiveservices.azure.com/'\n",
        "\n",
        "# Set the API URL\n",
        "analyze_url = endpoint + 'vision/v3.2/analyze'\n",
        "\n",
        "# Set the image URL\n",
        "image_url = '/content/meterw.jpg'\n",
        "\n",
        "# Set the API parameters\n",
        "params = {\n",
        "    'visualFeatures': 'Categories,Description',\n",
        "    'details': 'Celebrities',\n",
        "    'language': 'en'\n",
        "}\n",
        "\n",
        "# Set the API headers\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Ocp-Apim-Subscription-Key': subscription_key\n",
        "}\n",
        "\n",
        "# Call the API\n",
        "response = requests.post(analyze_url, headers=headers, params=params, json={'url': image_url})\n",
        "\n",
        "# Parse the response\n",
        "response_json = json.loads(response.content)\n",
        "\n",
        "# Print the response\n",
        "print(response_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcU0k4uyF1C-",
        "outputId": "bf91a1ab-b2a4-42b0-a42c-b9a66aff8308"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error': {'code': 'InvalidArgument', 'innererror': {'code': 'NotSupportedVisualFeature', 'message': 'Specified feature type is not valid'}, 'message': 'Specified feature type is not valid'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Set the API key and endpoint URL\n",
        "subscription_key = '3095b4d700184ffb81f5f991c9ee51ab'\n",
        "endpoint = 'https://siddhikottawar232.cognitiveservices.azure.com/'\n",
        "\n",
        "\n",
        "# Set the API URL\n",
        "ocr_url = endpoint + 'vision/v3.2/ocr'\n",
        "\n",
        "# Set the image URL\n",
        "image_url = '/content/meterw.jpg'\n",
        "\n",
        "# Set the API parameters\n",
        "params = {\n",
        "    'language': 'en',\n",
        "    'detectOrientation': 'true',\n",
        "    'visualFeatures': 'TextRecognition'\n",
        "}\n",
        "\n",
        "# Set the API headers\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Ocp-Apim-Subscription-Key': subscription_key\n",
        "}\n",
        "\n",
        "# Call the API\n",
        "response = requests.post(ocr_url, headers=headers, params=params, json={'url': image_url})\n",
        "\n",
        "# Parse the response\n",
        "response_json = json.loads(response.content)\n",
        "print(response_json )\n",
        "# Extract the recognized text from the response\n",
        "recognized_text = ''\n",
        "for region in response_json['eastus']:\n",
        "    for line in region['lines']:\n",
        "        for word in line['words']:\n",
        "            recognized_text += word['text'] + ' '\n",
        "\n",
        "# Print the recognized text\n",
        "print('Recognized text:', recognized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "YPM8TYvFMMRp",
        "outputId": "e859358a-0458-4e13-fe4e-ff2ae02d3636"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error': {'code': 'InvalidArgument', 'innererror': {'code': 'InvalidImageUrl', 'message': 'Image URL is badly formatted.'}, 'message': 'Image URL is badly formatted.'}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f4b4dfeccab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Extract the recognized text from the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mrecognized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eastus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eastus'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure.cognitiveservices.vision.computervision\n",
        "!pip install msrest.authentication\n",
        "import os\n",
        "import requests\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "# Set your API credentials and endpoint\n",
        "subscription_key = \"3095b4d700184ffb81f5f991c9ee51ab\"\n",
        "endpoint = \"https://siddhikottawar232.cognitiveservices.azure.com/\"\n",
        "\n",
        "# Set up a client object with the credentials and endpoint\n",
        "credentials = CognitiveServicesCredentials(subscription_key)\n",
        "client = ComputerVisionClient(endpoint, credentials)\n",
        "\n",
        "# Set the URL of the image containing the LPG meter reading\n",
        "image_url = \"/content/meterw.jpg\"\n",
        "\n",
        "# Set the feature types to extract\n",
        "features = [\"TextRecognition\"]\n",
        "\n",
        "# Call the Computer Vision API to analyze the image\n",
        "result = client.analyze_image(image_url, features)\n",
        "\n",
        "# Get the recognized text from the API response\n",
        "recognized_text = result.recognition_result.lines\n",
        "\n",
        "# Loop through the recognized text to find the LPG meter reading\n",
        "for line in recognized_text:\n",
        "    if \"LPG\" in line.text and \"Meter\" in line.text:\n",
        "        # Extract the LPG meter reading from the line\n",
        "        lpg_reading = line.text.split()[-1]\n",
        "        print(\"LPG meter reading:\", lpg_reading)\n",
        "        break"
      ],
      "metadata": {
        "id": "v3YXXSOXNkw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qsx0VA_8LLSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-cognitiveservices-vision-computervision\n",
        "\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "import os\n",
        "region = 'eastus'\n",
        "key = '3095b4d700184ffb81f5f991c9ee51ab'\n",
        "\n",
        "credentials = CognitiveServicesCredentials(key)\n",
        "client = ComputerVisionClient(\n",
        "    endpoint=\"https://siddhikottawar232.cognitiveservices.azure.com/\",\n",
        "    credentials=credentials\n",
        ")\n",
        "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Broadway_and_Times_Square_by_night.jpg/450px-Broadway_and_Times_Square_by_night.jpg\"\n",
        "\n",
        "image_analysis = client.analyze_image(url,visual_features=[VisualFeatureTypes.tags])\n",
        "\n",
        "for tag in image_analysis.tags:\n",
        "    print(tag)\n",
        "\n",
        "models = client.list_models()\n",
        "\n",
        "for x in models.models_property:\n",
        "    print(x)\n",
        "\n",
        "domain = \"landmarks\"\n",
        "url = \"https://images.pexels.com/photos/338515/pexels-photo-338515.jpeg\"\n",
        "language = \"en\"\n",
        "\n",
        "analysis = client.analyze_image_by_domain(domain, url, language)\n",
        "\n",
        "# for landmark in analysis.result[\"landmarks\"]:\n",
        "#     print(landmark[\"name\"])\n",
        "#     print(landmark[\"confidence\"])\n",
        "\n",
        "domain = \"landmarks\"\n",
        "url = \"/content/meterw.jpg\"\n",
        "language = \"en\"\n",
        "max_descriptions = 3\n",
        "\n",
        "analysis = client.describe_image(url, max_descriptions, language)\n",
        "\n",
        "for caption in analysis.captions:\n",
        "    print(caption.text)\n",
        "    print(caption.confidence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "hs5tgHxvSReo",
        "outputId": "8bd84ea1-7bb6-4373-e449-24badbb37d44"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: azure-cognitiveservices-vision-computervision in /usr/local/lib/python3.8/dist-packages (0.9.0)\n",
            "Requirement already satisfied: msrest>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from azure-cognitiveservices-vision-computervision) (0.7.1)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.8/dist-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)\n",
            "Requirement already satisfied: azure-core>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.26.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2022.12.7)\n",
            "Requirement already satisfied: requests~=2.16 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.25.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (0.6.1)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.2.2)\n",
            "{'additional_properties': {}, 'name': 'building', 'confidence': 0.9917298555374146, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'metropolis', 'confidence': 0.9371380805969238, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'metropolitan area', 'confidence': 0.9317947626113892, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'downtown', 'confidence': 0.928309440612793, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'skyscraper', 'confidence': 0.9235161542892456, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'outdoor', 'confidence': 0.9170207977294922, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'urban area', 'confidence': 0.913856029510498, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'street', 'confidence': 0.8924431204795837, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'commercial building', 'confidence': 0.8783712387084961, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'crowded', 'confidence': 0.8758255243301392, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'mixed-use', 'confidence': 0.8670334815979004, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'night', 'confidence': 0.8203572034835815, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'city', 'confidence': 0.8055667877197266, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'people', 'confidence': 0.6955504417419434, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'light', 'confidence': 0.6881226301193237, 'hint': None}\n",
            "{'additional_properties': {}, 'name': 'celebrities', 'categories': ['people_', '人_', 'pessoas_', 'gente_']}\n",
            "{'additional_properties': {}, 'name': 'landmarks', 'categories': ['outdoor_', '户外_', '屋外_', 'aoarlivre_', 'alairelibre_', 'building_', '建筑_', '建物_', 'edifício_']}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ComputerVisionErrorResponseException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mComputerVisionErrorResponseException\u001b[0m      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ff02d902b0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmax_descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/azure/cognitiveservices/vision/computervision/operations/_computer_vision_client_operations.py\u001b[0m in \u001b[0;36mdescribe_image\u001b[0;34m(self, url, max_candidates, language, description_exclude, model_version, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComputerVisionErrorResponseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mdeserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mComputerVisionErrorResponseException\u001b[0m: (InvalidArgument) Image URL is badly formatted."
          ]
        }
      ]
    }
  ]
}